{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gc\n",
        "\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NHwItDH9gddl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    TextStreamer,\n",
        "    pipeline,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "from huggingface_hub import login, notebook_login\n",
        "from IPython.display import display, Markdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Ozhi5tPtgn0U"
      },
      "outputs": [],
      "source": [
        "def print_markdown(text: str):\n",
        "  return display(Markdown(text))\n",
        "model_id=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "998b2556aced48aba9ab08bb76f2e166",
            "7234d0f46f8a43a194c542f5446b4e6a",
            "9ac1a8b1bb264fc3b438a3984ed1a72e",
            "daadc7677b244b15a8f36db5fd9f31ae",
            "cdbc42c8132046f69c87c2e64aa496a1",
            "6d2f0b6757254b0ebb316a7786943476",
            "c970f6cdb0054e69bd86b4cb1987bafd",
            "c4d2dcc08a374677857f24b3dae098a6",
            "45512218c0954f0cb9547bf00dc494f6",
            "8a6ab6f09b8840a9b9bbfd895fd99f3a",
            "a4acb11c7c9548fab831a42cf0117d8e"
          ]
        },
        "id": "5xngXE5fgqdF",
        "outputId": "d5ce39d8-5475-4647-fcde-d73b862209e2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ae278de6c73b4f7abdc2d819b78b5297",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/339 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "# quantization settings\n",
        "quantization_configs = BitsAndBytesConfig(\n",
        "  load_in_4bit=True,\n",
        "  bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "  bnb_4bit_quant_type='nf4',\n",
        "  bnb_4bit_use_double_quant=True\n",
        ")\n",
        "\n",
        "torch.device(\"mps\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "  model_id,\n",
        "  attn_implementation=\"sdpa\",\n",
        "  quantization_config=quantization_configs,\n",
        "  device_map=\"auto\",\n",
        "  trust_remote_code=True,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
        "\n",
        "prompts = [\n",
        "  \"if (5x - 10) = 2, solve for x\",\n",
        "  \"if x = [1 0 2] and y = [0 1 0] find x^T y\",\n",
        "  r\"\"\"\n",
        "**Advanced Linear Algebra — Singular Value Decomposition (SVD) Computation**\n",
        "Let\n",
        "\\[\n",
        "A =\n",
        "\\begin{bmatrix}\n",
        "4 & 0 & 2 \\\\\n",
        "1 & 3 & 1 \\\\\n",
        "0 & 2 & 2\n",
        "\\end{bmatrix}.\n",
        "\\]\n",
        "\n",
        "Compute the full Singular Value Decomposition of \\(A\\). That is, determine orthogonal matrices \\(U \\in \\mathbb{R}^{3 \\times 3}\\) and \\(V \\in \\mathbb{R}^{3 \\times 3}\\), along with a diagonal matrix \\(\\Sigma \\in \\mathbb{R}^{3 \\times 3}\\) with non-negative entries, such that\n",
        "\n",
        "\\[\n",
        "A = U \\Sigma V^\\top.\n",
        "\\]\n",
        "\n",
        "Your computation must proceed from first principles: derive the singular values via the eigenvalues of \\(A^\\top A\\), construct \\(V\\) from the corresponding orthonormal eigenvectors, compute \\(U\\) using \\(U = A V \\Sigma^{-1}\\), and explicitly verify the decomposition by matrix multiplication. Finally, use the SVD to determine the rank and 2-norm condition number of \\(A\\).\n",
        "\"\"\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "yXyjg6kjguvS"
      },
      "outputs": [],
      "source": [
        "# inputs = [tokenizer(prompt, return_tensors='pt').to('cuda') for prompt in prompts]\n",
        "# outputs = model.generate(**inputs[-1], max_new_tokens=1500, streamer=streamer, return_dict_in_generate=True)\n",
        "# response = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n",
        "\n",
        "# parse chain-of-thought tokens\n",
        "\n",
        "def process_response(response):\n",
        "  reason_start = response.find('<think>') + len('<think>')\n",
        "  reason_end = response.find('</think>')\n",
        "  reason = response[reason_start:reason_end].strip()\n",
        "\n",
        "  # output\n",
        "  output_start = reason_end + len('</think>')\n",
        "  output = response[output_start:].strip()\n",
        "\n",
        "  return reason, output\n",
        "\n",
        "# reason, output = process_response(response)\n",
        "# print_markdown(f\"**Reasoning**: {reason}\")\n",
        "# print_markdown(f\"**Output**: {output}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RKnFSRs-IK2"
      },
      "source": [
        "### Let's fine-tune Deepseek-R1-Qwen 1.5 b on math & physics problems\n",
        "\n",
        "Datasets:\n",
        "\n",
        "<b>Math</b>\n",
        "```{python}\n",
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"math-ai/StackMathQA\", \"stackmathqa1600k\") # or any valid config_name\n",
        "```\n",
        "\n",
        "<b> General 'science' datasets </b>\n",
        "<li> Scibench </li>\n",
        "<li> Others </li>\n",
        "\n",
        "```{python}\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset_names = [('xw27/scibench', 'xw27/scibench'), (name, config)]\n",
        "for ds_name, config_name in dataset_names:\n",
        "  ds = load_dataset(ds_name, config_name)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444,
          "referenced_widgets": [
            "7c93676aa71344dd9a008bf3bf0abfe2",
            "7122f6f376c444bea1bfa0c363bbed11",
            "26e199ed4786476f8ef177e46cd6b0b8",
            "fbd520decd8f42c2add8a023e500be14",
            "6931691cb5b6448cb4d507dd8a95adaf",
            "b2b427aeccc04d54aece405fde777edf",
            "65883bf052724088b766cf4f924d8219",
            "568b6989eaa54553aba024b13bd270c2",
            "2a1fcf10a70e413d8f470654c7122c9d",
            "238aaa244b184cb18c1d8d743d2f0e00",
            "b06155a627b346e1add53a8ec10131ab"
          ]
        },
        "id": "CnjHrvL4-QfF",
        "outputId": "7e38160b-9fd0-47cc-d375-a70b9add36a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading mathllm/MathCodeInstruct...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "13edcc00e253495188c508195a315c17",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train_80k.jsonl:   0%|          | 0.00/186M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from datasets import load_dataset, concatenate_datasets\n",
        "\n",
        "# Load datasets\n",
        "print(\"Loading mathllm/MathCodeInstruct...\")\n",
        "math_dataset = load_dataset(\"mathllm/MathCodeInstruct\", split=\"train\")\n",
        "\n",
        "print(\"Loading xw27/scibench...\")\n",
        "scibench_dataset = load_dataset(\"xw27/scibench\", split=\"train\")\n",
        "\n",
        "# Display dataset sizes\n",
        "print(f\"\\nOriginal dataset sizes:\")\n",
        "print(f\"  MathCodeInstruct: {len(math_dataset):,} examples\")\n",
        "print(f\"  SciBench: {len(scibench_dataset):,} examples\")\n",
        "\n",
        "# Calculate sampling to get ~25k total examples\n",
        "# Allocate 60% to MathCodeInstruct (for tool-calling) and 40% to SciBench (for science)\n",
        "max_total = 25000\n",
        "math_samples = min(len(math_dataset), int(max_total * 0.6))  # 15k for tool-calling & math\n",
        "scibench_samples = min(len(scibench_dataset), max_total - math_samples)  # ~10k for science\n",
        "\n",
        "# Sample from datasets\n",
        "print(f\"\\nSampling datasets to reach ~{max_total:,} examples...\")\n",
        "print(f\"  Target: {math_samples:,} from MathCodeInstruct (tool-calling & math)\")\n",
        "print(f\"  Target: {scibench_samples:,} from SciBench (science)\")\n",
        "\n",
        "math_subset = math_dataset.shuffle(seed=42).select(range(math_samples))\n",
        "scibench_subset = scibench_dataset.shuffle(seed=42).select(range(scibench_samples))\n",
        "\n",
        "# Combine datasets\n",
        "print(\"\\nCombining datasets...\")\n",
        "combined_dataset = concatenate_datasets([math_subset, scibench_subset])\n",
        "\n",
        "# Shuffle the combined dataset\n",
        "combined_dataset = combined_dataset.shuffle(seed=42)\n",
        "\n",
        "print(f\"\\n Combined dataset created:\")\n",
        "print(f\"  Total examples: {len(combined_dataset):,}\")\n",
        "print(f\"  - From MathCodeInstruct (tool-calling & math): {math_samples:,} ({math_samples/len(combined_dataset)*100:.1f}%)\")\n",
        "print(f\"  - From SciBench (science): {scibench_samples:,} ({scibench_samples/len(combined_dataset)*100:.1f}%)\")\n",
        "print(f\"\\nDataset features: {combined_dataset.features}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-xuIpyiEYIa"
      },
      "source": [
        "# Fine-Tuning Setup\n",
        "\n",
        "Now we'll fine-tune DeepSeek-R1-Distill-Qwen-1.5B on our mixed math/science dataset using QLoRA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RwNIjayUTsh5",
        "outputId": "492bf0bf-6d22-42c5-cd7b-4f01dfdb72d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "system: [{'type': 'text', 'content': 'Below is a math problem. Please solve it step by step.'}]\n",
            "\n",
            "user: [{'type': 'text', 'content': 'A soccer team has three goalies and ten defenders. The team also has twice as many midfielders as defenders, and the rest of the players are strikers. If the team has 40 players, how many strikers are in the team?'}]\n",
            "\n",
            "assistant: [{'type': 'code', 'content': '# Given:\\ngoalies = 3\\ndefenders = 10\\nmidfielders = 2 * defenders  # twice as many midfielders as defenders\\n\\n# Total players in the team is 40\\ntotal_players = 40\\n\\n# The rest of the players are strikers, so:\\nstrikers = total_players - (goalies + defenders + midfielders)\\nstrikers'}, {'type': 'execution', 'content': '7'}, {'type': 'text', 'content': 'The number of strikers in the team is \\\\(\\\\boxed{7}\\\\).'}, {'type': 'text', 'content': '7'}]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "first_item = combined_dataset[0]\n",
        "\n",
        "message = first_item[\"messages\"]\n",
        "for msg in message:\n",
        "  print(f\"{msg['role']}: {msg['content']}\")\n",
        "  print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FW7uDaY0EYIa"
      },
      "outputs": [],
      "source": [
        "# Install fine-tuning dependencies\n",
        "!pip install -q -U \\\n",
        "    peft>=0.14.0 \\\n",
        "    trl>=0.12.0 \\\n",
        "    accelerate>=1.2.0 \\\n",
        "    scipy \\\n",
        "    tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGIB0zjoEYIb",
        "outputId": "f937b959-1107-44e1-f688-dd40fd0f0d46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Formatting datasets to chat format with <think> reasoning tags...\n",
            "Formatting MathCodeInstruct...\n",
            "Formatting SciBench...\n",
            "  MathCodeInstruct: 15,000 valid examples\n",
            "  SciBench: 112 valid examples\n",
            "\n",
            "✓ Formatted dataset: 15,112 examples\n",
            "\n",
            "Example:\n",
            "{'messages': [{'content': '[{\\'type\\': \\'text\\', \\'content\\': \"Chloe bought chocolate-dipped strawberries at $50 a dozen. She then sold them for $30 for half a dozen during the Mother\\'s Day celebration. How much is Chloe\\'s profit if she sold 50 dozens?\"}]', 'role': 'user'}, {'content': \"<think>\\nAlright, let's break the problem down step-by-step:\\n\\n1. **Determine Chloe's expense for 1 dozen strawberries:**\\n   Chloe bought strawberries for $50 a dozen.\\n\\n2. **Determine Chloe's revenue from selling 1 dozen strawberries:**\\n   Chloe sells them for $30 for half a dozen. \\n   So, for 1 dozen, she would sell them for \\\\(2 \\\\times 30\\\\).\\n\\n3. **Determine Chloe's profit for 1 dozen strawberries:**\\n   Profit for 1 dozen = Revenue from 1 dozen - Expense for 1 dozen \\n\\n4. **Determine Chloe's profit for 50 dozens:**\\n   Total profit = 50 x Profit for 1 dozen\\n\\nNow, let's calculate these step-by-step.\\n<code>\\n# 1. Determine Chloe's expense for 1 dozen strawberries\\nexpense_per_dozen = 50\\n\\n# 2. Determine Chloe's revenue from selling 1 dozen strawberries\\nrevenue_per_half_dozen = 30\\nrevenue_per_dozen = 2 * revenue_per_half_dozen\\n\\n# 3. Determine Chloe's profit for 1 dozen strawberries\\nprofit_per_dozen = revenue_per_dozen - expense_per_dozen\\n\\n# 4. Determine Chloe's profit for 50 dozens\\ntotal_profit = 50 * profit_per_dozen\\n\\ntotal_profit\\n</code>\\n500\\nChloe's profit for selling 50 dozens of chocolate-dipped strawberries is \\\\(\\\\boxed{\\\\$500}\\\\).\\n500\\n</think>\\n\\nThe solution is provided above.\", 'role': 'assistant'}]}\n"
          ]
        }
      ],
      "source": [
        "from datasets import Dataset, concatenate_datasets\n",
        "\n",
        "def format_mathcodeinstruct(example):\n",
        "    \"\"\"Convert MathCodeInstruct to DeepSeek-R1 chat format with reasoning.\"\"\"\n",
        "    messages = example.get(\"messages\", [])\n",
        "    if not messages:\n",
        "        return None\n",
        "\n",
        "    user_msg = None\n",
        "    assistant_msg = None\n",
        "\n",
        "    for msg in messages:\n",
        "        if msg.get('role') == 'user':\n",
        "            user_msg = msg.get('content', '')\n",
        "        elif msg.get('role') == 'assistant':\n",
        "            assistant_msg = msg.get('content', '')\n",
        "\n",
        "    if not user_msg or not assistant_msg:\n",
        "        return None\n",
        "\n",
        "    formatted_string = \"\"\n",
        "    if isinstance(assistant_msg, list):\n",
        "        for block in assistant_msg:\n",
        "            if isinstance(block, dict):\n",
        "                if block.get('type') == 'code':\n",
        "                    formatted_string += f\"<code>\\n{block.get('content', '')}\\n</code>\\n\"\n",
        "                else:\n",
        "                    formatted_string += f\"{block.get('content', '')}\\n\"\n",
        "            else:\n",
        "                formatted_string += str(block) + \"\\n\"\n",
        "        formatted_string = f\"<think>\\n{formatted_string.strip()}\\n</think>\\n\\nThe solution is provided above.\"\n",
        "    else:\n",
        "        formatted_string = f\"<think>\\n{assistant_msg}\\n</think>\\n\\nThe solution is provided above.\"\n",
        "\n",
        "    return {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"user\", \"content\": str(user_msg)},\n",
        "            {\"role\": \"assistant\", \"content\": formatted_string}\n",
        "        ]\n",
        "    }\n",
        "\n",
        "\n",
        "def format_scibench(example):\n",
        "    \"\"\"Convert SciBench to DeepSeek-R1 chat format with reasoning.\"\"\"\n",
        "    problem = example.get('problem_text', '')\n",
        "    solution = example.get('solution', '')\n",
        "    answer = example.get('answer_latex', '')\n",
        "    unit = example.get('unit', '')\n",
        "\n",
        "    if not problem or not solution:\n",
        "        return None\n",
        "\n",
        "    final_answer = answer\n",
        "    if unit:\n",
        "        final_answer = f\"{answer} {unit}\"\n",
        "\n",
        "    formatted_response = f\"<think>\\n{solution}\\n</think>\\n\\nFinal Answer: {final_answer}\"\n",
        "\n",
        "    return {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"user\", \"content\": str(problem)},\n",
        "            {\"role\": \"assistant\", \"content\": formatted_response}\n",
        "        ]\n",
        "    }\n",
        "\n",
        "\n",
        "print(\"Formatting datasets to chat format with <think> reasoning tags...\")\n",
        "\n",
        "# Build lists directly — no .map(), no Arrow type issues\n",
        "print(\"Formatting MathCodeInstruct...\")\n",
        "math_rows = []\n",
        "for example in math_subset:\n",
        "    result = format_mathcodeinstruct(example)\n",
        "    if result is not None:\n",
        "        math_rows.append(result)\n",
        "\n",
        "print(\"Formatting SciBench...\")\n",
        "sci_rows = []\n",
        "for example in scibench_subset:\n",
        "    result = format_scibench(example)\n",
        "    if result is not None:\n",
        "        sci_rows.append(result)\n",
        "\n",
        "# Create datasets from consistent lists\n",
        "math_formatted = Dataset.from_list(math_rows)\n",
        "scibench_formatted = Dataset.from_list(sci_rows)\n",
        "\n",
        "print(f\"  MathCodeInstruct: {len(math_formatted):,} valid examples\")\n",
        "print(f\"  SciBench: {len(scibench_formatted):,} valid examples\")\n",
        "\n",
        "# Combine and shuffle\n",
        "formatted_dataset = concatenate_datasets([math_formatted, scibench_formatted])\n",
        "formatted_dataset = formatted_dataset.shuffle(seed=42)\n",
        "\n",
        "print(f\"\\n Formatted dataset: {len(formatted_dataset):,} examples\")\n",
        "print(\"\\nExample:\")\n",
        "print(formatted_dataset[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DnoTAMMFEYIb",
        "outputId": "22848317-af3e-43ff-b9f2-202a7cda58aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating train/eval split (90/10)...\n",
            "\n",
            "✓ Split complete:\n",
            "  Training examples: 13,600\n",
            "  Evaluation examples: 1,512\n"
          ]
        }
      ],
      "source": [
        "# Create 90/10 train/eval split\n",
        "print(\"Creating train/eval split (90/10)...\")\n",
        "split_dataset = formatted_dataset.train_test_split(test_size=0.1, seed=42)\n",
        "\n",
        "train_dataset = split_dataset['train']\n",
        "eval_dataset = split_dataset['test']\n",
        "\n",
        "print(f\"\\n✓ Split complete:\")\n",
        "print(f\"  Training examples: {len(train_dataset):,}\")\n",
        "print(f\"  Evaluation examples: {len(eval_dataset):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAj6Jk8oEYIb",
        "outputId": "629770c1-fe39-464c-e369-b9a2962360a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model and tokenizer loaded\n"
          ]
        }
      ],
      "source": [
        "# Load model for fine-tuning with QLoRA\n",
        "from peft import (\n",
        "    prepare_model_for_kbit_training,\n",
        "    LoraConfig,\n",
        "    get_peft_model\n",
        ")\n",
        "\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "  tokenizer.pad_token = tokenizer.eos_token\n",
        "  tokenizer.padding_side = \"right\"  # Important for training\n",
        "  model.config.pad_token_id = tokenizer.pad_token\n",
        "\n",
        "# Prepare model for k-bit training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# LoRA version\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
        "    ],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CASUAL_LM\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# tokenize dataset?\n",
        "def tokenize(examples):\n",
        "  return tokenizer.apply_chat_template(\n",
        "      examples[\"messages\"],\n",
        "      truncation=True\n",
        "      )\n",
        "\n",
        "tokenized_training_dataset = train_dataset.map(\n",
        "    tokenize,\n",
        "    batched=True,\n",
        "    remove_columns=train_dataset.column_names\n",
        ")\n",
        "\n",
        "tokenized_eval_dataset = test_dataset.map(\n",
        "    tokenize,\n",
        "    batched=True,\n",
        "    remove_columns=test_dataset.column_names\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer, mlm=False\n",
        ") # next-token prediction\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./deepseek-r1-math-science-lora-fine-tuning\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=2,\n",
        "    max_steps=-1, # -1 value ensures that it follows the number of training epochs param\n",
        "\n",
        "    # batch configs\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    gradient_accumulation_steps=8,\n",
        "    gradient_checkpointing=True,\n",
        "\n",
        "    # learning rate configs\n",
        "    learning_rate=2e-4, # generally 5e-5 for full fine-tuning\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.10,\n",
        "\n",
        "    # optimization configs\n",
        "    optim=\"paged_adamw_8bit\", # memory efficient, adamw_torch_fused is fastest\n",
        "    weight_decay=0.1,\n",
        "    max_grad_norm=0.1,\n",
        "\n",
        "    # eval configs\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=500,\n",
        "\n",
        "    # saving configs\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=500,\n",
        "    save_total_limit=3,\n",
        "\n",
        "    # Logging configs\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=50,\n",
        "    logging_first_step=True,\n",
        "\n",
        "    # performance configs\n",
        "    fp16=True,\n",
        "    dataloader_num_workers=4,\n",
        "    dataloader_pin_memory=True,\n",
        "\n",
        "    # Misc\n",
        "    report_to=\"none\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        ")\n",
        "\n",
        "# initialize trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_training_dataset,\n",
        "    eval_dataset=tokenized_eval_dataset,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "# train\n",
        "print(\"TRAINING STARTED\")\n",
        "trainer.train()\n",
        "print(\"TRAINING COMPLETE\")\n",
        "\n",
        "\n",
        "# save model\n",
        "output_path = \"./deepseek-r1-math-science-full-fine-tuning\"\n",
        "trainer.save_model(output_path)\n",
        "tokenizer.save_pretrained(output_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlkQRru9EYId"
      },
      "source": [
        "# Testing Fine-Tuned Model\n",
        "\n",
        "Let's test the fine-tuned model on some math and science problems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRv570T2EYId"
      },
      "source": [
        "# Merge LoRA Adapters with Base Model\n",
        "\n",
        "For deployment, you can merge the LoRA adapters with the base model to create a standalone model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QfvKmd-oEYId"
      },
      "outputs": [],
      "source": [
        "# Optional: Merge LoRA adapters with base model for deployment\n",
        "print(\"Merging LoRA adapters with base model...\")\n",
        "print(\"This creates a standalone model (~3GB) that doesn't require PEFT.\\n\")\n",
        "\n",
        "# Reload base model in fp16 (not quantized) for merging\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "\n",
        "# Load base model in fp16\n",
        "merge_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "# Load LoRA adapters\n",
        "merge_model = PeftModel.from_pretrained(merge_model, output_path)\n",
        "\n",
        "# Merge adapters into base model\n",
        "merged_model = merge_model.merge_and_unload()\n",
        "\n",
        "# Save merged model\n",
        "merged_output_path = \"./deepseek-r1-math-science-merged\"\n",
        "merged_model.save_pretrained(merged_output_path)\n",
        "tokenizer.save_pretrained(merged_output_path)\n",
        "\n",
        "print(f\"\\n Merged model saved to {merged_output_path}\")\n",
        "print(f\"  Size: ~3GB\")\n",
        "print(f\"\\nYou can now convert this to GGUF for on-device deployment using llama.cpp.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ob25kI09IoD1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PvFm9mXnK7F"
      },
      "source": [
        "# Visualise Attention\n",
        "\n",
        "<p> Use TinyBERT & visualizew the self attention mechanism </p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tNvrrpJvnWqX"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModel\n",
        "from bertviz import head_view, model_view"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mzLnFHxngwK"
      },
      "outputs": [],
      "source": [
        "model=AutoModel.from_pretrained(\"prajjwal1/bert-tiny\", output_attentions=True)\n",
        "tokenizer=AutoTokenizer.from_pretrained(\"prajjwal1/bert-tiny\")\n",
        "\n",
        "prompt = \"John's dog ate Nemis's cat\"\n",
        "inputs = tokenizer(prompt, return_tensors='pt')\n",
        "with torch.no_grad():\n",
        "  outputs = model(**inputs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-L7oX0JwinVz"
      },
      "source": [
        "# Stop doing this blindly:\n",
        "model = AutoModelForCausalLM.from_pretrained(\"model\")\n",
        "\n",
        "#### Start understanding:\n",
        "#### - What are attention heads doing?\n",
        "#### - How does positional encoding work?\n",
        "#### - What's the difference between encoder-only (BERT), decoder-only (GPT), encoder-decoder (T5)?\n",
        "#### - Why does model size scale quadratically with context length?\n",
        "\n",
        "Action items:\n",
        "\n",
        "Read \"Attention Is All You Need\" paper (seriously)\n",
        "Implement a mini-transformer from scratch (100-200 lines)\n",
        "Use model.generate(output_attentions=True) to visualize what the model \"sees\"\n",
        "Study the HuggingFace modeling_*.py source code for your favorite models\n",
        "Resources:\n",
        "\n",
        "Andrej Karpathy's \"Neural Networks: Zero to Hero\" (YouTube)\n",
        "Jay Alammar's \"Illustrated Transformer\" blog\n",
        "HuggingFace Transformers source code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfssHiGupxVU"
      },
      "source": [
        "#### Bootcamp level:\n",
        "model = AutoModelForCausalLM.from_pretrained(\"model\", load_in_4bit=True)\n",
        "\n",
        "#### Production level:\n",
        "#### - Batching multiple requests efficiently\n",
        "#### - KV-cache optimization for faster inference\n",
        "#### - Flash Attention for 2-4x speedups\n",
        "#### - ONNX/TensorRT conversion for production serving\n",
        "#### - Model distillation to create smaller models\n",
        "#### - Prompt caching to reduce redundant computation\n",
        "\n",
        "Learn:\n",
        "\n",
        "vLLM / Text Generation Inference (TGI): Production inference servers\n",
        "GGML/llama.cpp: CPU-optimized inference\n",
        "Triton Inference Server: Multi-model deployment\n",
        "Batch processing: Processing 100 requests > processing 1 request 100 times\n",
        "Continuous batching: Dynamic batching as requests arrive\n",
        "Metrics that matter:\n",
        "\n",
        "Throughput (tokens/second)\n",
        "Latency (time to first token, time per token)\n",
        "Cost per 1M tokens\n",
        "GPU utilization %"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7Wxuf6Yip-H"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "jupyterfix",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "238aaa244b184cb18c1d8d743d2f0e00": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26e199ed4786476f8ef177e46cd6b0b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_568b6989eaa54553aba024b13bd270c2",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2a1fcf10a70e413d8f470654c7122c9d",
            "value": 20
          }
        },
        "2a1fcf10a70e413d8f470654c7122c9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "45512218c0954f0cb9547bf00dc494f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "568b6989eaa54553aba024b13bd270c2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65883bf052724088b766cf4f924d8219": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6931691cb5b6448cb4d507dd8a95adaf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d2f0b6757254b0ebb316a7786943476": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7122f6f376c444bea1bfa0c363bbed11": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2b427aeccc04d54aece405fde777edf",
            "placeholder": "​",
            "style": "IPY_MODEL_65883bf052724088b766cf4f924d8219",
            "value": "Resolving data files: 100%"
          }
        },
        "7234d0f46f8a43a194c542f5446b4e6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d2f0b6757254b0ebb316a7786943476",
            "placeholder": "​",
            "style": "IPY_MODEL_c970f6cdb0054e69bd86b4cb1987bafd",
            "value": "Loading weights: 100%"
          }
        },
        "7c93676aa71344dd9a008bf3bf0abfe2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7122f6f376c444bea1bfa0c363bbed11",
              "IPY_MODEL_26e199ed4786476f8ef177e46cd6b0b8",
              "IPY_MODEL_fbd520decd8f42c2add8a023e500be14"
            ],
            "layout": "IPY_MODEL_6931691cb5b6448cb4d507dd8a95adaf"
          }
        },
        "8a6ab6f09b8840a9b9bbfd895fd99f3a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "998b2556aced48aba9ab08bb76f2e166": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7234d0f46f8a43a194c542f5446b4e6a",
              "IPY_MODEL_9ac1a8b1bb264fc3b438a3984ed1a72e",
              "IPY_MODEL_daadc7677b244b15a8f36db5fd9f31ae"
            ],
            "layout": "IPY_MODEL_cdbc42c8132046f69c87c2e64aa496a1"
          }
        },
        "9ac1a8b1bb264fc3b438a3984ed1a72e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c4d2dcc08a374677857f24b3dae098a6",
            "max": 339,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_45512218c0954f0cb9547bf00dc494f6",
            "value": 339
          }
        },
        "a4acb11c7c9548fab831a42cf0117d8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b06155a627b346e1add53a8ec10131ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b2b427aeccc04d54aece405fde777edf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4d2dcc08a374677857f24b3dae098a6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c970f6cdb0054e69bd86b4cb1987bafd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cdbc42c8132046f69c87c2e64aa496a1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "daadc7677b244b15a8f36db5fd9f31ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a6ab6f09b8840a9b9bbfd895fd99f3a",
            "placeholder": "​",
            "style": "IPY_MODEL_a4acb11c7c9548fab831a42cf0117d8e",
            "value": " 339/339 [00:00&lt;00:00, 741.79it/s, Materializing param=model.norm.weight]"
          }
        },
        "fbd520decd8f42c2add8a023e500be14": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_238aaa244b184cb18c1d8d743d2f0e00",
            "placeholder": "​",
            "style": "IPY_MODEL_b06155a627b346e1add53a8ec10131ab",
            "value": " 20/20 [00:00&lt;00:00, 3907.68it/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
